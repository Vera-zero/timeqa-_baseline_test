#!/usr/bin/env python3
"""
QA Results Evaluation Script
è¯„ä¼° QA-result ç›®å½•ä¸­çš„æ‰€æœ‰æ–¹æ³•ç»“æœï¼Œè®¡ç®— EM å’Œ F1 åˆ†æ•°
"""

import os
import json
import re
import string
import argparse
from collections import Counter, defaultdict
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
from unidecode import unidecode


# ============================================================================
# è¯„ä¼°æŒ‡æ ‡æ¨¡å— (å‚è€ƒ utils.py)
# ============================================================================

def normalize_answer(s: str) -> str:
    """æ ‡å‡†åŒ–ç­”æ¡ˆæ–‡æœ¬"""
    def remove_articles(text):
        return re.sub(r"\b(a|an|the)\b", " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def replace_dash_with_space(text):
        return " ".join(text.split("-"))

    def remove_punc(text):
        exclude = set(string.punctuation)
        return "".join([ch for ch in text if ch not in exclude])

    def lower(text):
        if isinstance(text, (int, float)):
            text = str(text)
        return unidecode(text.lower())

    return white_space_fix(remove_articles(remove_punc(replace_dash_with_space(lower(s)))))


def f1_score(prediction: str, ground_truth: str) -> Tuple[float, float, float]:
    """è®¡ç®— token çº§åˆ«çš„ F1 åˆ†æ•°"""
    ZERO_METRIC = (0, 0, 0)

    if prediction in ['yes', 'no', 'noanswer'] and prediction != ground_truth:
        return ZERO_METRIC
    if ground_truth in ['yes', 'no', 'noanswer'] and prediction != ground_truth:
        return ZERO_METRIC

    prediction_tokens = prediction.split()
    ground_truth_tokens = ground_truth.split()
    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0:
        return ZERO_METRIC

    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)

    return f1, precision, recall


def get_metrics(preds: List[str], gt_answer: List[str]) -> Dict[str, float]:
    """è®¡ç®— EM å’Œ F1 æŒ‡æ ‡"""
    if isinstance(gt_answer, str):
        gt_answer = [gt_answer]
    if isinstance(preds, str):
        preds = [preds]

    if len(preds) == 0 and len(gt_answer) != 0:
        return {'em': 0, 'f1': 0}
    if len(preds) != 0 and len(gt_answer) == 0:
        return {'em': 0, 'f1': 0}

    em = 0
    f1 = 0

    for pred in preds:
        pred = normalize_answer(pred)
        if pred == "":
            if gt_answer[0] == "":
                return {'em': 1, 'f1': 1.0}
            else:
                return {'em': 0, 'f1': 0}

        for gt in gt_answer:
            gt = normalize_answer(gt)
            em = max(em, int(pred == gt))
            f1 = max(f1, f1_score(pred, gt)[0])
            if em:
                return {'em': 1, 'f1': 1.0}

    return {'em': em, 'f1': f1}


# ============================================================================
# ç­”æ¡ˆæå–æ¨¡å—
# ============================================================================

def extract_bold_entity_from_first_sentence(text: str) -> str:
    """ä»ç¬¬ä¸€å¥è¯ä¸­æå–åŠ ç²—å®ä½“ï¼ˆ**å®ä½“å**ï¼‰"""
    if not text:
        return ""

    # æ‰¾åˆ°ç¬¬ä¸€å¥è¯ï¼ˆä»¥./?/!ç»“æŸï¼‰
    first_sentence_match = re.split(r'[.!?]', text)
    first_sentence = first_sentence_match[0] if first_sentence_match else text

    # æå–åŠ ç²—å®ä½“ **XXX**
    bold_pattern = r'\*\*([^*]+)\*\*'
    matches = re.findall(bold_pattern, first_sentence)

    if matches:
        return matches[0].strip()

    # å¦‚æœæ²¡æœ‰åŠ ç²—å®ä½“ï¼Œè¿”å›ç¬¬ä¸€å¥è¯
    return first_sentence.strip()


def extract_answer_from_dyg_rag(answer_text: str) -> str:
    """ä» DyG-RAG çš„ answer å­—æ®µä¸­æå–ç­”æ¡ˆ"""
    if not answer_text:
        return ""

    # æŸ¥æ‰¾ **Answer:** åçš„å†…å®¹
    answer_pattern = r'\*\*Answer:\*\*\s*(.*?)(?:\*\*Justification:\*\*|$)'
    answer_match = re.search(answer_pattern, answer_text, re.DOTALL | re.IGNORECASE)

    if answer_match:
        answer_section = answer_match.group(1).strip()
        # ä» Answer éƒ¨åˆ†æå–ç¬¬ä¸€ä¸ªåŠ ç²—å®ä½“
        return extract_bold_entity_from_first_sentence(answer_section)

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ° Answer: æ ‡è®°ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªåŠ ç²—å®ä½“
    bold_pattern = r'\*\*([^*]+)\*\*'
    matches = re.findall(bold_pattern, answer_text)
    if matches:
        # è¿‡æ»¤æ‰ "Answer:", "Justification:" ç­‰æ ‡è®°
        for match in matches:
            if match.lower() not in ['answer:', 'justification:', 'answer', 'justification']:
                return match.strip()

    return ""


def extract_answer_by_method(method_name: str, result_data: Dict) -> str:
    """æ ¹æ®æ–¹æ³•åæå–ç­”æ¡ˆ"""
    if method_name == "DyG-RAG":
        # DyG-RAG: ä» answer å­—æ®µçš„ Answer: éƒ¨åˆ†æå–
        answer_text = result_data.get('answer', '')
        return extract_answer_from_dyg_rag(answer_text)
    else:
        # å…¶ä»–æ–¹æ³•: ä» output çš„ç¬¬ä¸€å¥è¯æå–åŠ ç²—å®ä½“
        output_text = result_data.get('output', '')
        return extract_bold_entity_from_first_sentence(output_text)


# ============================================================================
# æ•°æ®åŠ è½½æ¨¡å—
# ============================================================================

def load_jsonl(file_path: str) -> List[Dict]:
    """åŠ è½½ JSONL æ ¼å¼æ–‡ä»¶"""
    results = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                results.append(json.loads(line))
    return results


def load_json(file_path: str) -> Dict:
    """åŠ è½½ JSON æ ¼å¼æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_method_results(method_path: str, method_name: str) -> List[Dict]:
    """åŠ è½½æŸä¸ªæ–¹æ³•çš„ç»“æœæ–‡ä»¶"""
    results_file = os.path.join(method_path, 'results.json')

    if not os.path.exists(results_file):
        print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶ {results_file}")
        return []

    # å°è¯•ä½œä¸º JSON åŠ è½½
    try:
        data = load_json(results_file)
        if isinstance(data, dict) and 'results' in data:
            # DyG-RAG æ ¼å¼
            return data['results']
        elif isinstance(data, list):
            return data
    except json.JSONDecodeError:
        pass

    # å°è¯•ä½œä¸º JSONL åŠ è½½
    try:
        return load_jsonl(results_file)
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½ {results_file}: {e}")
        return []


def load_ground_truth(dataset_path: str) -> Dict[str, Dict]:
    """åŠ è½½ ground truth æ•°æ®é›†ï¼Œè¿”å› question -> {targets, level} çš„æ˜ å°„"""
    gt_mapping = {}

    test_file = os.path.join(dataset_path, 'test_processed.json')
    if not os.path.exists(test_file):
        print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° ground truth æ–‡ä»¶ {test_file}")
        return gt_mapping

    data = load_json(test_file)

    if 'datas' in data:
        for doc in data['datas']:
            if 'questions_list' in doc:
                for q_data in doc['questions_list']:
                    question = q_data.get('question', '')
                    targets = q_data.get('targets', [])
                    level = q_data.get('level', 'unknown')
                    gt_mapping[question] = {
                        'targets': targets,
                        'level': level
                    }

    return gt_mapping


# ============================================================================
# è¯„ä¼°æ‰§è¡Œæ¨¡å—
# ============================================================================

def evaluate_method(method_name: str, method_path: str, dataset_name: str,
                   dataset_path: str) -> Dict[str, Any]:
    """è¯„ä¼°å•ä¸ªæ–¹æ³•"""
    print(f"\nğŸ” è¯„ä¼°æ–¹æ³•: {method_name} (æ•°æ®é›†: {dataset_name})")

    # åŠ è½½æ–¹æ³•ç»“æœ
    results = load_method_results(method_path, method_name)
    if not results:
        print(f"  âš ï¸  è·³è¿‡ {method_name}: æ— ç»“æœæ•°æ®")
        return None

    # åŠ è½½ ground truth
    gt_mapping = load_ground_truth(dataset_path)

    # è¯„ä¼°æ¯ä¸ªé—®é¢˜
    evaluation_results = []

    for result in results:
        # æå–é—®é¢˜å’Œé¢„æµ‹ç­”æ¡ˆ
        question = result.get('question', '')
        predicted_answer = extract_answer_by_method(method_name, result)

        # è·å– ground truth
        if question in gt_mapping:
            gt_data = gt_mapping[question]
            gt_answers = gt_data['targets']
            level = gt_data['level']
        else:
            # HippoRAG ç­‰æ–¹æ³•è‡ªå¸¦ ground truth
            gt_answers = result.get('answer', [])
            level = result.get('level', 'unknown')

        # è®¡ç®—æŒ‡æ ‡
        metrics = get_metrics([predicted_answer], gt_answers)

        evaluation_results.append({
            'question': question,
            'predicted': predicted_answer,
            'ground_truth': gt_answers,
            'level': level,
            'em': metrics['em'],
            'f1': metrics['f1']
        })

    print(f"  âœ… è¯„ä¼°äº† {len(evaluation_results)} ä¸ªé—®é¢˜")

    return {
        'method': method_name,
        'dataset': dataset_name,
        'results': evaluation_results
    }


def aggregate_results(evaluation_data: List[Dict]) -> Dict:
    """èšåˆè¯„ä¼°ç»“æœï¼ŒæŒ‰æ–¹æ³•ã€æ•°æ®é›†ã€çº§åˆ«åˆ†ç»„"""
    aggregated = defaultdict(lambda: {
        'em_scores': [],
        'f1_scores': [],
        'count': 0
    })

    for eval_result in evaluation_data:
        if not eval_result:
            continue

        method = eval_result['method']
        dataset = eval_result['dataset']

        for result in eval_result['results']:
            level = result['level']
            em = result['em']
            f1 = result['f1']

            # æŒ‰çº§åˆ«èšåˆ
            key = (method, dataset, level)
            aggregated[key]['em_scores'].append(em)
            aggregated[key]['f1_scores'].append(f1)
            aggregated[key]['count'] += 1

            # æ€»ä½“èšåˆ
            overall_key = (method, dataset, 'overall')
            aggregated[overall_key]['em_scores'].append(em)
            aggregated[overall_key]['f1_scores'].append(f1)
            aggregated[overall_key]['count'] += 1

    # è®¡ç®—å¹³å‡å€¼
    final_results = []
    for (method, dataset, level), data in aggregated.items():
        em_avg = sum(data['em_scores']) / len(data['em_scores']) * 100
        f1_avg = sum(data['f1_scores']) / len(data['f1_scores']) * 100

        final_results.append({
            'method': method,
            'dataset': dataset,
            'level': level,
            'em': em_avg,
            'f1': f1_avg,
            'count': data['count']
        })

    return final_results


# ============================================================================
# è¡¨æ ¼ç”Ÿæˆæ¨¡å—
# ============================================================================

def generate_markdown_table(results: List[Dict], output_file: str):
    """ç”Ÿæˆ Markdown æ ¼å¼è¡¨æ ¼"""
    lines = []
    lines.append("# QA è¯„ä¼°ç»“æœ\n")
    lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append("\n## è¯¦ç»†ç»“æœ\n")

    # è¡¨å¤´
    lines.append("| Method | Dataset | Level | EM (%) | F1 (%) | Questions |")
    lines.append("|--------|---------|-------|--------|--------|-----------|")

    # æŒ‰æ–¹æ³•å’Œæ•°æ®é›†åˆ†ç»„ï¼Œå…ˆæ˜¾ç¤ºå­çº§åˆ«ï¼Œå†æ˜¾ç¤º overall
    grouped = defaultdict(list)
    for r in results:
        key = (r['method'], r['dataset'])
        grouped[key].append(r)

    for (method, dataset), items in sorted(grouped.items()):
        # å…ˆæ˜¾ç¤º easy/hard ç­‰å­çº§åˆ«
        sub_levels = [item for item in items if item['level'] != 'overall']
        for item in sorted(sub_levels, key=lambda x: x['level']):
            lines.append(
                f"| {item['method']} | {item['dataset']} | {item['level']} | "
                f"{item['em']:.2f} | {item['f1']:.2f} | {item['count']} |"
            )

        # æ˜¾ç¤º overallï¼ˆåŠ ç²—ï¼‰
        overall_items = [item for item in items if item['level'] == 'overall']
        for item in overall_items:
            lines.append(
                f"| **{item['method']}** | **{item['dataset']}** | **{item['level']}** | "
                f"**{item['em']:.2f}** | **{item['f1']:.2f}** | **{item['count']}** |"
            )

    # æ–¹æ³•å¯¹æ¯”ï¼ˆOverallï¼‰
    lines.append("\n## æ–¹æ³•å¯¹æ¯” (Overall)\n")
    lines.append("| Method | Avg EM (%) | Avg F1 (%) |")
    lines.append("|--------|------------|------------|")

    method_overall = defaultdict(lambda: {'em': [], 'f1': []})
    for r in results:
        if r['level'] == 'overall':
            method_overall[r['method']]['em'].append(r['em'])
            method_overall[r['method']]['f1'].append(r['f1'])

    method_comparison = []
    for method, data in method_overall.items():
        avg_em = sum(data['em']) / len(data['em'])
        avg_f1 = sum(data['f1']) / len(data['f1'])
        method_comparison.append((method, avg_em, avg_f1))

    # æŒ‰ F1 é™åºæ’åˆ—
    for method, avg_em, avg_f1 in sorted(method_comparison, key=lambda x: x[2], reverse=True):
        lines.append(f"| {method} | {avg_em:.2f} | {avg_f1:.2f} |")

    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

    print(f"\nâœ… Markdown è¡¨æ ¼å·²ä¿å­˜åˆ°: {output_file}")


def generate_csv_table(results: List[Dict], output_file: str):
    """ç”Ÿæˆ CSV æ ¼å¼è¡¨æ ¼"""
    import csv

    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Method', 'Dataset', 'Level', 'EM (%)', 'F1 (%)', 'Questions'])

        for r in sorted(results, key=lambda x: (x['method'], x['dataset'], x['level'])):
            writer.writerow([
                r['method'],
                r['dataset'],
                r['level'],
                f"{r['em']:.2f}",
                f"{r['f1']:.2f}",
                r['count']
            ])

    print(f"âœ… CSV è¡¨æ ¼å·²ä¿å­˜åˆ°: {output_file}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼° QA ç»“æœ')
    parser.add_argument('--qa-result-dir', default='/workspace/ETE-Graph/QA-result',
                       help='QA-result ç›®å½•è·¯å¾„')
    parser.add_argument('--dataset-dir', default='/workspace/ETE-Graph/dataset',
                       help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', default='/workspace/ETE-Graph/evaluation_results',
                       help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--dataset', default=None,
                       help='ä»…è¯„ä¼°æŒ‡å®šæ•°æ®é›†')
    parser.add_argument('--method', default=None,
                       help='ä»…è¯„ä¼°æŒ‡å®šæ–¹æ³•')
    parser.add_argument('--output-format', default='both', choices=['markdown', 'csv', 'both'],
                       help='è¾“å‡ºæ ¼å¼')

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    print("=" * 70)
    print("QA ç»“æœè¯„ä¼°è„šæœ¬")
    print("=" * 70)

    # æ‰«ææ‰€æœ‰æ•°æ®é›†å’Œæ–¹æ³•
    qa_result_path = Path(args.qa_result_dir)
    evaluation_data = []

    for dataset_dir in qa_result_path.iterdir():
        if not dataset_dir.is_dir():
            continue

        dataset_name = dataset_dir.name

        # è¿‡æ»¤æ•°æ®é›†
        if args.dataset and dataset_name != args.dataset:
            continue

        dataset_path = os.path.join(args.dataset_dir, dataset_name)

        for method_dir in dataset_dir.iterdir():
            if not method_dir.is_dir():
                continue

            method_name = method_dir.name

            # è¿‡æ»¤æ–¹æ³•
            if args.method and method_name != args.method:
                continue

            # è¯„ä¼°
            eval_result = evaluate_method(
                method_name, str(method_dir), dataset_name, dataset_path
            )
            if eval_result:
                evaluation_data.append(eval_result)

    if not evaluation_data:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯„ä¼°æ•°æ®")
        return

    # èšåˆç»“æœ
    print("\nğŸ“Š èšåˆè¯„ä¼°ç»“æœ...")
    aggregated_results = aggregate_results(evaluation_data)

    # ç”Ÿæˆè¡¨æ ¼
    if args.output_format in ['markdown', 'both']:
        md_file = os.path.join(args.output_dir, 'results_table.md')
        generate_markdown_table(aggregated_results, md_file)

    if args.output_format in ['csv', 'both']:
        csv_file = os.path.join(args.output_dir, 'results_table.csv')
        generate_csv_table(aggregated_results, csv_file)

    print("\n" + "=" * 70)
    print("âœ… è¯„ä¼°å®Œæˆ!")
    print("=" * 70)


if __name__ == '__main__':
    main()
