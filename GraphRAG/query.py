from Core.GraphRAG import GraphRAG
from Option.Config2 import Config
import argparse
import os
import asyncio
import time
import json
from pathlib import Path
from shutil import copyfile
from datetime import datetime
from Data.QueryDataset import RAGQueryDataset
import pandas as pd
from Core.Utils.Evaluation import Evaluator



def check_dirs(opt):
    # working_dir æ˜¯ä¸­é—´æ–‡ä»¶ç›®å½•ï¼š/workspace/ETE-Graph/workdir/{dataset_name}/{method_name}
    # result_dir æ˜¯æœ€ç»ˆç»“æœç›®å½•ï¼š/workspace/ETE-Graph/QA-result/{dataset_name}/{method_name}
    config_dir = os.path.join(opt.working_dir, "Configs")
    metric_dir = os.path.join(opt.working_dir, "Metrics")

    os.makedirs(config_dir, exist_ok=True)
    os.makedirs(metric_dir, exist_ok=True)

    # ç¡®ä¿ result_dirï¼ˆQA-result ç›®å½•ï¼‰å­˜åœ¨ï¼Œç”¨äºä¿å­˜ results.json
    os.makedirs(opt.result_dir, exist_ok=True)

    # æå–é…ç½®æ–‡ä»¶å
    method_config_name = Path(args.opt).name  # å¦‚ "HippoRAG.yaml"
    base_config_path = Path(args.opt).parent.parent / "Config2.yaml"

    # å¤åˆ¶é…ç½®æ–‡ä»¶åˆ° working_dir/Configs
    copyfile(args.opt, os.path.join(config_dir, method_config_name))
    if base_config_path.exists():
        copyfile(base_config_path, os.path.join(config_dir, "Config2.yaml"))

    return metric_dir  # è¿”å› Metrics ç›®å½•ç”¨äºä¿å­˜æŒ‡æ ‡


def wrapper_query(query_dataset, digimon, result_dir, opt):
    """
    åŸºæœ¬æŸ¥è¯¢å‡½æ•°ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    """
    # Checkpoint support - åŠ è½½å·²æœ‰ç»“æœ
    save_path = os.path.join(opt.result_dir, "results.json")
    existing_results = []
    processed_indices = set()

    if os.path.exists(save_path):
        try:
            # è¯»å–å·²æœ‰çš„ç»“æœæ–‡ä»¶
            with open(save_path, 'r', encoding='utf-8') as f:
                for line in f:
                    result = json.loads(line.strip())
                    existing_results.append(result)
                    # ä½¿ç”¨é—®é¢˜ç´¢å¼•ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    question_idx = result.get('question_idx')
                    if question_idx is not None:
                        processed_indices.add(question_idx)
            print(f"\nğŸ“‚ å‘ç°å·²æœ‰ç»“æœæ–‡ä»¶ï¼Œå·²å®Œæˆ {len(processed_indices)} ä¸ªé—®é¢˜")
        except Exception as e:
            print(f"\nâš ï¸  è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°å¼€å§‹")
            existing_results = []
            processed_indices = set()

    all_res = existing_results.copy()

    dataset_len = len(query_dataset)
    dataset_len = 10

    save_interval = 5  # æ¯5ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡
    questions_since_last_save = 0
    skip_mode = False  # æ ‡è®°æ˜¯å¦è¿›å…¥è·³è¿‡æ¨¡å¼

    print(f"\nå¼€å§‹å¤„ç† {dataset_len} ä¸ªé—®é¢˜...")

    for _, i in enumerate(range(dataset_len)):
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡æ­¤é—®é¢˜
        if i in processed_indices:
            if not skip_mode:
                print(f"\nâœ“ é—®é¢˜ {i} å·²å¤„ç†ï¼Œè·³è¿‡...")
                skip_mode = True
            continue

        # ä¸€æ—¦å‘ç°æœªå¤„ç†çš„é—®é¢˜ï¼Œè¯´æ˜ä»æ­¤ä¹‹åéƒ½æœªå¤„ç†
        if skip_mode:
            print(f"\nâ†’ ä»é—®é¢˜ {i} å¼€å§‹ç»§ç»­å¤„ç†...")
            skip_mode = False

        query = query_dataset[i]
        start_time = time.time()
        res = asyncio.run(digimon.query(query["question"]))
        end_time = time.time()
        query_time = end_time - start_time

        # æ·»åŠ é—®é¢˜ç´¢å¼•ç”¨äºæ–­ç‚¹ç»­ä¼ 
        query["question_idx"] = i
        query["output"] = res
        query["query_time"] = query_time
        all_res.append(query)
        processed_indices.add(i)
        questions_since_last_save += 1

        # æ¯5ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡
        if questions_since_last_save >= save_interval:
            all_res_df = pd.DataFrame(all_res)
            all_res_df.to_json(save_path, orient="records", lines=True)
            print(f"\nğŸ’¾ å·²ä¿å­˜è¿›åº¦: {len(processed_indices)}/{dataset_len} ä¸ªé—®é¢˜")
            questions_since_last_save = 0

    # æœ€ç»ˆä¿å­˜æ‰€æœ‰ç»“æœ
    all_res_df = pd.DataFrame(all_res)
    all_res_df.to_json(save_path, orient="records", lines=True)
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
    print(f"   - å¤„ç†é—®é¢˜æ•°: {len(processed_indices)}/{dataset_len}")
    return save_path


def wrapper_query_filtered(filtered_questions, digimon, result_dir, opt):
    """
    æŸ¥è¯¢å·²ç»ç­›é€‰è¿‡çš„é—®é¢˜åˆ—è¡¨ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰

    Args:
        filtered_questions: å·²ç­›é€‰çš„é—®é¢˜åˆ—è¡¨
        digimon: GraphRAGå®ä¾‹
        result_dir: ç»“æœä¿å­˜ç›®å½•ï¼ˆè¿™é‡Œç”¨äºmetricsï¼‰
        opt: é…ç½®å¯¹è±¡
    """
    # Checkpoint support - åŠ è½½å·²æœ‰ç»“æœ
    save_path = os.path.join(opt.result_dir, "results.json")
    existing_results = []
    processed_indices = set()

    if os.path.exists(save_path):
        try:
            # è¯»å–å·²æœ‰çš„ç»“æœæ–‡ä»¶
            with open(save_path, 'r', encoding='utf-8') as f:
                for line in f:
                    result = json.loads(line.strip())
                    existing_results.append(result)
                    # ä½¿ç”¨é—®é¢˜ç´¢å¼•ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    question_idx = result.get('question_idx')
                    if question_idx is not None:
                        processed_indices.add(question_idx)
            print(f"\nğŸ“‚ å‘ç°å·²æœ‰ç»“æœæ–‡ä»¶ï¼Œå·²å®Œæˆ {len(processed_indices)} ä¸ªé—®é¢˜")
        except Exception as e:
            print(f"\nâš ï¸  è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°å¼€å§‹")
            existing_results = []
            processed_indices = set()

    all_res = existing_results.copy()
    save_interval = 5  # æ¯5ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡
    questions_since_last_save = 0
    skip_mode = False  # æ ‡è®°æ˜¯å¦è¿›å…¥è·³è¿‡æ¨¡å¼

    print(f"\nå¼€å§‹å¤„ç† {len(filtered_questions)} ä¸ªé—®é¢˜...")

    for idx, query in enumerate(filtered_questions):
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡æ­¤é—®é¢˜
        if idx in processed_indices:
            if not skip_mode:
                print(f"\nâœ“ é—®é¢˜ {idx} å·²å¤„ç†ï¼Œè·³è¿‡...")
                skip_mode = True
            continue

        # ä¸€æ—¦å‘ç°æœªå¤„ç†çš„é—®é¢˜ï¼Œè¯´æ˜ä»æ­¤ä¹‹åéƒ½æœªå¤„ç†
        if skip_mode:
            print(f"\nâ†’ ä»é—®é¢˜ {idx} å¼€å§‹ç»§ç»­å¤„ç†...")
            skip_mode = False

        doc_id = query.get('doc_id', 'N/A')
        print(f"\n[{idx+1}/{len(filtered_questions)}] æ–‡æ¡£{doc_id}: {query['question'][:60]}...")

        start_time = time.time()
        res = asyncio.run(digimon.query(query["question"]))
        end_time = time.time()
        query_time = end_time - start_time

        # æ·»åŠ é—®é¢˜ç´¢å¼•ç”¨äºæ–­ç‚¹ç»­ä¼ 
        query["question_idx"] = idx
        query["output"] = res
        query["query_time"] = query_time
        all_res.append(query)
        processed_indices.add(idx)
        questions_since_last_save += 1

        print(f"  å›ç­”: {res[:100]}...")
        print(f"  æŸ¥è¯¢è€—æ—¶: {query_time:.2f}ç§’")

        # æ¯5ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡
        if questions_since_last_save >= save_interval:
            all_res_df = pd.DataFrame(all_res)
            all_res_df.to_json(save_path, orient="records", lines=True)
            print(f"\nğŸ’¾ å·²ä¿å­˜è¿›åº¦: {len(processed_indices)}/{len(filtered_questions)} ä¸ªé—®é¢˜")
            questions_since_last_save = 0

    # æœ€ç»ˆä¿å­˜æ‰€æœ‰ç»“æœ
    all_res_df = pd.DataFrame(all_res)
    all_res_df.to_json(save_path, orient="records", lines=True)
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
    print(f"   - å¤„ç†é—®é¢˜æ•°: {len(processed_indices)}/{len(filtered_questions)}")
    return save_path


async def wrapper_evaluation(path, opt, result_dir):
    eval = Evaluator(path, opt.dataset_name)
    res_dict = await eval.evaluate()
    save_path = os.path.join(result_dir, "metrics.json")
    with open(save_path, "w") as f:
        f.write(str(res_dict))


if __name__ == "__main__":

    # with open("./book.txt") as f:
    #     doc = f.read()

    parser = argparse.ArgumentParser()
    parser.add_argument("-opt", type=str, help="Path to option YMAL file.")
    parser.add_argument("-dataset_name", type=str, help="Name of the dataset.")
    parser.add_argument("-data_root", type=str, default=None,
                        help="Root directory for datasets (overrides config file).")
    parser.add_argument("-file_pattern", type=str, default=None,
                        help="Specific data file name (e.g., 'test_processed.json').")
    args = parser.parse_args()

    opt = Config.parse(Path(args.opt), dataset_name=args.dataset_name, data_root=args.data_root)
    digimon = GraphRAG(config=opt)
    result_dir = check_dirs(opt)

    query_dataset = RAGQueryDataset(
        data_dir=os.path.join(opt.data_root, opt.dataset_name),
        file_pattern=args.file_pattern
    )

    # åªä½¿ç”¨å‰2ä¸ªæ–‡æ¡£
    corpus = query_dataset.get_corpus()
    corpus = corpus[:2]
    print(f"ä½¿ç”¨å‰ {len(corpus)} ä¸ªæ–‡æ¡£:")
    for doc in corpus:
        print(f"  - doc_id={doc['doc_id']}: {doc['title']}")

    # ç­›é€‰å‡ºå±äºè¿™2ä¸ªæ–‡æ¡£çš„é—®é¢˜
    doc_ids = {doc['doc_id'] for doc in corpus}
    filtered_questions = [
        query_dataset[i]
        for i in range(len(query_dataset))
        if query_dataset[i].get('doc_id') in doc_ids
    ]
    print(f"\nè¿™äº›æ–‡æ¡£å¯¹åº” {len(filtered_questions)} ä¸ªé—®é¢˜")

    # # æ’å…¥å‰2ä¸ªæ–‡æ¡£
    # asyncio.run(digimon.insert(corpus))

    # æŸ¥è¯¢å±äºè¿™2ä¸ªæ–‡æ¡£çš„é—®é¢˜
    save_path = wrapper_query_filtered(filtered_questions, digimon, result_dir, opt)

    # # è¯„ä¼°ç»“æœ
    # asyncio.run(wrapper_evaluation(save_path, opt, result_dir))

    # for train_item in dataloader:

    # a = asyncio.run(digimon.query("Who is Fred Gehrke?"))

    # asyncio.run(digimon.query("Who is Scrooge?"))
