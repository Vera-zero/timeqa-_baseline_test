# ETE-Graph Agent 方法运行指南

本文档介绍如何运行 `/workspace/ETE-Graph/agent` 目录中的四种时序问答方法。

## 目录结构

```
agent/
├── MRAG-master/           # MRAG 方法原代码
├── timeqa_baseline_lab/   # 包含 zero_shot_cot、rag_cot、react 三种方法
└── ReAct-master/         # ReAct 原始实现（已迁移到 timeqa_baseline_lab）
```

---

## 方法一：MRAG（Modular Retrieval-Augmented Generation）

### 简介
MRAG 是一个模块化的时序问答检索框架，使用原始代码实现。

### 位置
[MRAG-master](MRAG-master/)

### 运行步骤

#### 1. 环境准备
```bash
cd /workspace/ETE-Graph/agent/MRAG-master
```

#### 2. 设置 CUDA 设备
在 [reader.py](MRAG-master/reader.py) 中配置 GPU：
```python
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"  # 根据实际GPU情况修改
```

#### 3. 运行命令
```bash
python reader.py \
    --max-examples 100 \
    --retriever-output "situatedqa_contriever_metriever_bgegemma_llama_8b_qfs5_outputs.json" \
    --ctx-topk 3 \
    --stage1-model contriever \
    --reader gpt \
    --paradigm concat
```

#### 4. 主要参数说明
| 参数 | 说明 | 可选值 |
|------|------|--------|
| `--max-examples` | 最大样本数量 | 整数 |
| `--retriever-output` | 检索器输出文件名 | JSON 文件名 |
| `--ctx-topk` | 使用的上下文数量 | 整数（默认：3） |
| `--stage1-model` | 第一阶段检索模型 | `bm25`, `contriever`, `hybrid` |
| `--reader` | 阅读器模型 | `llama`, `llama_8b`, `llama_70b`, `gpt` |
| `--paradigm` | 阅读范式 | `fusion`, `concat` |
| `--param-pred` | 是否启用参数预测 | 布尔值（默认：True） |
| `--param-cot` | 是否启用 CoT 参数预测 | 布尔值（默认：False） |

#### 5. 输出位置
- 结果文件保存在 `./answered/` 目录
- 文件名格式：`{retriever_name}_top{ctx_topk}_{llm_name}_{paradigm}_results.csv`

---

## 方法二：Zero-Shot Chain-of-Thought (zero_shot_cot)

### 简介
零样本思维链方法，直接让 LLM 逐步推理回答问题，不使用检索到的上下文。

### 位置
[timeqa_baseline_lab](timeqa_baseline_lab/)

### 运行步骤

#### 1. 环境准备
```bash
cd /workspace/ETE-Graph/agent/timeqa_baseline_lab
```

#### 2. 配置文件修改
编辑 [configs/default.yaml](timeqa_baseline_lab/configs/default.yaml)，修改以下关键配置：

```yaml
data:
  corpus_path: "/path/to/corpus/test.json"           # 修改为实际语料库路径
  question_arrow_path: "/path/to/test/data.arrow"    # 修改为实际问题数据路径

model:
  provider: "local"                                   # 或 "remote"
  model_name: "Qwen/Qwen3-4B-Instruct-2507"          # 本地模型名称

run:
  strategy: "zero_shot_cot"                          # 设置策略为 zero_shot_cot
  max_questions: 100                                 # 处理的问题数量
```

#### 3. 运行命令
```bash
# 使用默认配置
python -m timeqa_baseline_lab.run

# 或者使用命令行覆盖参数
python -m timeqa_baseline_lab.run \
    --config configs/default.yaml \
    --strategy zero_shot_cot \
    --max_questions 100
```

#### 4. 方法特点
- **无需检索**：直接使用 LLM 的参数化知识
- **思维链提示**：要求模型逐步思考后给出答案
- **提示格式**：`"Question: {question}\nThink step by step silently, then output only:\nFinal Answer: <answer>"`

---

## 方法三：RAG Chain-of-Thought (rag_cot)

### 简介
基于检索增强的思维链方法，结合检索到的上下文和思维链推理。

### 位置
[timeqa_baseline_lab](timeqa_baseline_lab/)

### 运行步骤

#### 1. 环境准备
```bash
cd /workspace/ETE-Graph/agent/timeqa_baseline_lab
```

#### 2. 配置文件修改
编辑 [configs/default.yaml](timeqa_baseline_lab/configs/default.yaml)：

```yaml
data:
  corpus_path: "/path/to/corpus/test.json"
  question_arrow_path: "/path/to/test/data.arrow"

model:
  provider: "local"
  model_name: "Qwen/Qwen3-4B-Instruct-2507"

retriever:
  type: "contriever"
  model_name: "facebook/contriever"
  top_k: 5                                            # 检索的文档数量

run:
  strategy: "rag_cot"                                # 设置策略为 rag_cot
  max_questions: 100
```

#### 3. 运行命令
```bash
# 使用默认配置
python -m timeqa_baseline_lab.run

# 或者使用命令行覆盖参数
python -m timeqa_baseline_lab.run \
    --config configs/default.yaml \
    --strategy rag_cot \
    --max_questions 100
```

#### 4. 方法特点
- **检索增强**：首先使用 Contriever 检索相关文档
- **上下文整合**：将检索到的文档片段格式化为上下文
- **思维链推理**：基于上下文进行逐步推理
- **提示格式**：包含问题、上下文和思维链指令

---

## 方法四：ReAct (Reasoning + Acting)

### 简介
ReAct 方法通过交替执行推理（Thought）和行动（Action）来解决问题，支持多步检索和推理。

### 位置
[timeqa_baseline_lab](timeqa_baseline_lab/)（代码已从 ReAct-master 迁移）

### 运行步骤

#### 1. 环境准备
```bash
cd /workspace/ETE-Graph/agent/timeqa_baseline_lab
```

#### 2. 配置文件修改
编辑 [configs/default.yaml](timeqa_baseline_lab/configs/default.yaml)：

```yaml
data:
  corpus_path: "/path/to/corpus/test.json"
  question_arrow_path: "/path/to/test/data.arrow"

model:
  provider: "local"
  model_name: "Qwen/Qwen3-4B-Instruct-2507"

retriever:
  type: "contriever"
  model_name: "facebook/contriever"
  top_k: 5

run:
  strategy: "react"                                  # 设置策略为 react
  max_questions: 100
  strategy_params:
    max_steps: 6                                     # ReAct 最大步数
```

#### 3. 运行命令
```bash
# 使用默认配置
python -m timeqa_baseline_lab.run

# 或者使用命令行覆盖参数
python -m timeqa_baseline_lab.run \
    --config configs/default.yaml \
    --strategy react \
    --max_questions 100
```

#### 4. 方法特点
- **交互式推理**：Thought（思考）→ Action（行动）→ Observation（观察）循环
- **支持的行动**：
  - `Search[entity]`：检索相关文档
  - `Lookup[keyword]`：在当前文档中查找关键词
  - `Finish[answer]`：给出最终答案
- **最大步数**：默认 6 步（可在配置中修改）
- **Few-shot 示例**：使用预定义的示例引导模型

---

## 通用说明

### 环境依赖安装

#### timeqa_baseline_lab 依赖
```bash
cd /workspace/ETE-Graph/agent/timeqa_baseline_lab
pip install -r requirements.txt
```

主要依赖：
- PyYAML >= 6.0
- torch >= 2.1
- transformers >= 4.45
- accelerate >= 0.33
- pyarrow >= 15.0
- tqdm >= 4.66

#### MRAG-master 依赖
```bash
cd /workspace/ETE-Graph/agent/MRAG-master
# 根据实际需要安装依赖（项目中可能需要 vLLM、OpenAI 等）
```

### 输出文件位置

#### timeqa_baseline_lab 输出
- 默认输出目录：`./outputs/`
- 缓存目录：`./cache/`
- 配置可在 [configs/default.yaml](timeqa_baseline_lab/configs/default.yaml) 的 `io` 部分修改

#### MRAG 输出
- 结果目录：`./answered/`
- 格式：CSV 文件，包含预测答案、准确率、F1 分数等

### 数据准备

确保以下数据文件存在：
1. **语料库文件**（corpus）：包含 Wikipedia 文档的 JSON 文件
2. **问题数据文件**：Arrow 格式的问题数据
3. **检索结果文件**（仅 MRAG）：预先检索的结果 JSON 文件

### 模型选择

#### 本地模型 (provider: local)
- 支持 HuggingFace 模型
- 示例：`Qwen/Qwen3-4B-Instruct-2507`, `meta-llama/Llama-3-8B-Instruct`
- 需要 GPU 支持

#### 远程 API (provider: remote)
- 支持 OpenAI 兼容的 API
- 示例：`deepseek-chat`, `gpt-4`, `gpt-3.5-turbo`
- 需要配置 API key 环境变量

### 常见参数说明

| 参数 | 说明 | 适用方法 |
|------|------|----------|
| `max_questions` | 处理的最大问题数量（<= 0 表示全部） | 所有方法 |
| `top_k` | 检索文档数量 | rag_cot, react |
| `batch_size` | 批处理大小 | zero_shot_cot, rag_cot |
| `max_steps` | 最大推理步数 | react |
| `temperature` | 生成温度（0.0 表示贪婪解码） | 所有方法 |
| `max_new_tokens` | 最大生成 token 数 | 所有方法 |

### 评估指标

所有方法都会输出以下评估指标：
- **Accuracy**：精确匹配准确率
- **F1 Score**：Token 级别的 F1 分数
- **分数据集评估**：TimeQA 和 SituatedQA 分别评估

---

## 快速开始示例

### 示例 1：运行 zero_shot_cot
```bash
cd /workspace/ETE-Graph/agent/timeqa_baseline_lab
python -m timeqa_baseline_lab.run \
    --strategy zero_shot_cot \
    --max_questions 50
```

### 示例 2：运行 rag_cot
```bash
cd /workspace/ETE-Graph/agent/timeqa_baseline_lab
python -m timeqa_baseline_lab.run \
    --strategy rag_cot \
    --max_questions 50
```

### 示例 3：运行 react
```bash
cd /workspace/ETE-Graph/agent/timeqa_baseline_lab
python -m timeqa_baseline_lab.run \
    --strategy react \
    --max_questions 50
```

### 示例 4：运行 MRAG
```bash
cd /workspace/ETE-Graph/agent/MRAG-master
python reader.py \
    --max-examples 50 \
    --reader gpt \
    --ctx-topk 3
```

---

## 故障排除

### 常见问题

1. **CUDA 内存不足**
   - 减少 `batch_size`
   - 使用更小的模型
   - 减少 `max_new_tokens`

2. **找不到数据文件**
   - 检查 `configs/default.yaml` 中的路径配置
   - 确保路径使用绝对路径或正确的相对路径

3. **模型加载失败**
   - 检查 HuggingFace 模型名称是否正确
   - 确保有足够的磁盘空间下载模型
   - 检查网络连接（首次下载时）

4. **API 调用失败**
   - 检查 API key 环境变量是否设置
   - 验证 `base_url` 配置是否正确
   - 检查网络连接

---

## 参考文献

### MRAG
- 论文：[MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering](https://arxiv.org/abs/2412.15540)
- 数据集：[TempRAGEval on HuggingFace](https://huggingface.co/datasets/siyue/TempRAGEval)

### ReAct
- 论文：ReAct: Synergizing Reasoning and Acting in Language Models
- 原始实现：[ReAct-master](ReAct-master/)

---

## 联系方式

如有问题，请联系：
- MRAG 相关：Siyue Zhang (siyue001@e.ntu.edu.sg)
- 其他问题：查看各项目的 README 文档

---

**最后更新**：2026-02-15
